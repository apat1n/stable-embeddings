{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a.tliamov/anaconda3/envs/py38item/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.dataset import Dataset\n",
    "from src.postprocess_data import MinInteractionsFilter, IdsEncoder, SplitTrainValTest\n",
    "import os\n",
    "import faiss\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from implicit.bpr import BayesianPersonalizedRanking\n",
    "from src.metrics import user_recall, user_ap\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users: 6034\n",
      "items: 3125\n",
      "interactions: 574376\n",
      "density: 3.05%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (574_376, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>item_id</th><th>event_ts</th></tr><tr><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>0</td><td>924</td><td>978300760</td></tr><tr><td>0</td><td>2685</td><td>978300275</td></tr><tr><td>0</td><td>1835</td><td>978824291</td></tr><tr><td>0</td><td>1015</td><td>978302039</td></tr><tr><td>0</td><td>2218</td><td>978300719</td></tr><tr><td>0</td><td>515</td><td>978302268</td></tr><tr><td>0</td><td>714</td><td>978301368</td></tr><tr><td>0</td><td>516</td><td>978824268</td></tr><tr><td>0</td><td>733</td><td>978301752</td></tr><tr><td>0</td><td>1876</td><td>978302281</td></tr><tr><td>0</td><td>2308</td><td>978302124</td></tr><tr><td>0</td><td>816</td><td>978301753</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>6033</td><td>1551</td><td>956703977</td></tr><tr><td>6033</td><td>473</td><td>956715288</td></tr><tr><td>6033</td><td>844</td><td>964828799</td></tr><tr><td>6033</td><td>480</td><td>956704746</td></tr><tr><td>6033</td><td>1554</td><td>956716207</td></tr><tr><td>6033</td><td>1560</td><td>956704519</td></tr><tr><td>6033</td><td>847</td><td>957717322</td></tr><tr><td>6033</td><td>856</td><td>956704996</td></tr><tr><td>6033</td><td>861</td><td>956704887</td></tr><tr><td>6033</td><td>491</td><td>956704746</td></tr><tr><td>6033</td><td>863</td><td>956715648</td></tr><tr><td>6033</td><td>864</td><td>956715569</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (574_376, 3)\n",
       "┌─────────┬─────────┬───────────┐\n",
       "│ user_id ┆ item_id ┆ event_ts  │\n",
       "│ ---     ┆ ---     ┆ ---       │\n",
       "│ i64     ┆ i64     ┆ i64       │\n",
       "╞═════════╪═════════╪═══════════╡\n",
       "│ 0       ┆ 924     ┆ 978300760 │\n",
       "│ 0       ┆ 2685    ┆ 978300275 │\n",
       "│ 0       ┆ 1835    ┆ 978824291 │\n",
       "│ 0       ┆ 1015    ┆ 978302039 │\n",
       "│ …       ┆ …       ┆ …         │\n",
       "│ 6033    ┆ 861     ┆ 956704887 │\n",
       "│ 6033    ┆ 491     ┆ 956704746 │\n",
       "│ 6033    ┆ 863     ┆ 956715648 │\n",
       "│ 6033    ┆ 864     ┆ 956715569 │\n",
       "└─────────┴─────────┴───────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Dataset('ml-1m').get_data()\n",
    "\n",
    "min_iterations_filter = MinInteractionsFilter()\n",
    "ids_encoder = IdsEncoder()\n",
    "\n",
    "data = min_iterations_filter.transform(data)\n",
    "data = ids_encoder.fit_transform(data)\n",
    "\n",
    "n_users = len(data['user_id'].unique())\n",
    "n_items = len(data['item_id'].unique())\n",
    "n_interactions = len(data)\n",
    "print(f'users: {n_users}')\n",
    "print(f'items: {n_items}')\n",
    "print(f'interactions: {n_interactions}')\n",
    "print(f'density: {(n_interactions / n_users / n_items) * 100:.2f}%')\n",
    "\n",
    "splitter = SplitTrainValTest()\n",
    "train_df, val_df, test_df = splitter.transform(data)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4006 users in train\n",
      "1579 users in val\n",
      "1784 users in test\n"
     ]
    }
   ],
   "source": [
    "from src.postprocess_data import create_sparse_dataset, create_positives_dataset\n",
    "\n",
    "train_dataset = create_sparse_dataset(train_df)\n",
    "train_positives = create_positives_dataset(train_df)\n",
    "val_positives = create_positives_dataset(val_df)\n",
    "test_positives = create_positives_dataset(test_df)\n",
    "\n",
    "print(f'{len(train_positives.keys())} users in train')\n",
    "print(f'{len(val_positives.keys())} users in val')\n",
    "print(f'{len(test_positives.keys())} users in test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейный learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_lr(epoch, n_epochs, lr_start, lr_end):\n",
    "    p = epoch / n_epochs\n",
    "    return lr_start * (1 - p) + lr_end * p\n",
    "\n",
    "def cyclic_lr(epoch, n_epochs, lr_start, lr_end):\n",
    "    t_i = 1 / RESTART_EPOCHS * ( (epoch - 1) % RESTART_EPOCHS + 1)\n",
    "    return lr_start * (1 - t_i) + lr_end * t_i\n",
    "\n",
    "class SWACallback:\n",
    "    def __init__(self, type_lr=\"linear_lr\"):\n",
    "        self.user_factors_swa = None\n",
    "        self.item_factors_swa = None\n",
    "        self.recall_history = defaultdict(list)\n",
    "        \n",
    "        self.type_lr = type_lr\n",
    "        \n",
    "        self.n_models = 0\n",
    "        \n",
    "    def callback_fn(self, epoch, *_):\n",
    "        if self.type_lr == \"linear_lr\":\n",
    "            model.learning_rate = linear_lr(epoch, model.iterations, LR_START, LR_END)\n",
    "        elif self.type_lr == \"cyclic_lr\":\n",
    "            model.learning_rate = cyclic_lr(epoch, model.iterations, LR_START, LR_END)\n",
    "        \n",
    "        if (epoch + 1) % RESTART_EPOCHS == 0:\n",
    "            if self.user_factors_swa is None:\n",
    "                self.user_factors_swa = model.user_factors\n",
    "                self.item_factors_swa = model.item_factors\n",
    "            else:\n",
    "                self.n_models = epoch // RESTART_EPOCHS\n",
    "                self.user_factors_swa = (self.user_factors_swa * self.n_models + model.user_factors) / (self.n_models + 1)\n",
    "                self.item_factors_swa = (self.item_factors_swa * self.n_models + model.item_factors) / (self.n_models + 1)\n",
    "        \n",
    "        index = faiss.IndexFlatIP(DIM)\n",
    "        index.add(model.item_factors)\n",
    "        recs = index.search(model.user_factors, 200)[1]\n",
    "        \n",
    "        for k in [1, 10, 20]:\n",
    "            recall_list = []\n",
    "            for user_id, y_true in val_positives.items():\n",
    "                y_pred = [\n",
    "                    item_id for item_id in recs[user_id]\n",
    "                    if item_id not in train_positives.get(user_id, set())\n",
    "                ]\n",
    "                recall_list.append(user_recall(y_pred, y_true, k))\n",
    "#             self.recall_history[k].append(np.mean(recall_list))\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def get_model(seed):\n",
    "    return BayesianPersonalizedRanking(\n",
    "        iterations=EPOCHS, factors=(DIM - 1), random_state=seed,\n",
    "        learning_rate=LR_START, regularization=REG_FACTOR\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейный LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 150/150 [01:38<00:00,  1.52it/s, train_auc=89.64%, skipped=19.61%]\n",
      "/home/a.tliamov/stable-embeddings/src/metrics.py:41: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.sum([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 0.0261\n",
      "MAP@5 0.1743\n",
      "Recall@10 0.0473\n",
      "MAP@10 0.1392\n",
      "Recall@5 0.027\n",
      "MAP@5 0.1729\n",
      "Recall@10 0.048\n",
      "MAP@10 0.1412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 150/150 [01:37<00:00,  1.54it/s, train_auc=89.59%, skipped=19.57%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 0.0265\n",
      "MAP@5 0.1749\n",
      "Recall@10 0.0472\n",
      "MAP@10 0.1403\n",
      "Recall@5 0.0278\n",
      "MAP@5 0.175\n",
      "Recall@10 0.0484\n",
      "MAP@10 0.1401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 150/150 [01:37<00:00,  1.54it/s, train_auc=89.59%, skipped=19.57%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 0.0256\n",
      "MAP@5 0.1756\n",
      "Recall@10 0.0467\n",
      "MAP@10 0.1401\n",
      "Recall@5 0.0267\n",
      "MAP@5 0.1752\n",
      "Recall@10 0.0478\n",
      "MAP@10 0.1388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 150/150 [01:38<00:00,  1.52it/s, train_auc=89.65%, skipped=19.61%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 0.0257\n",
      "MAP@5 0.1757\n",
      "Recall@10 0.0467\n",
      "MAP@10 0.14\n",
      "Recall@5 0.0264\n",
      "MAP@5 0.1765\n",
      "Recall@10 0.0483\n",
      "MAP@10 0.1403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 150/150 [01:37<00:00,  1.54it/s, train_auc=89.58%, skipped=19.56%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 0.0264\n",
      "MAP@5 0.1739\n",
      "Recall@10 0.0463\n",
      "MAP@10 0.1386\n",
      "Recall@5 0.0273\n",
      "MAP@5 0.1733\n",
      "Recall@10 0.0477\n",
      "MAP@10 0.1386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 150/150 [01:37<00:00,  1.54it/s, train_auc=89.56%, skipped=19.46%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 0.0264\n",
      "MAP@5 0.1739\n",
      "Recall@10 0.0454\n",
      "MAP@10 0.1376\n",
      "Recall@5 0.0269\n",
      "MAP@5 0.1758\n",
      "Recall@10 0.0473\n",
      "MAP@10 0.1398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 150/150 [01:39<00:00,  1.51it/s, train_auc=89.59%, skipped=19.47%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 0.0267\n",
      "MAP@5 0.1737\n",
      "Recall@10 0.0456\n",
      "MAP@10 0.1382\n",
      "Recall@5 0.0267\n",
      "MAP@5 0.1745\n",
      "Recall@10 0.0474\n",
      "MAP@10 0.1383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 150/150 [01:37<00:00,  1.54it/s, train_auc=89.57%, skipped=19.59%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 0.0261\n",
      "MAP@5 0.1752\n",
      "Recall@10 0.0457\n",
      "MAP@10 0.1397\n",
      "Recall@5 0.0267\n",
      "MAP@5 0.1766\n",
      "Recall@10 0.0471\n",
      "MAP@10 0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 150/150 [01:36<00:00,  1.55it/s, train_auc=89.67%, skipped=19.58%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 0.0254\n",
      "MAP@5 0.1699\n",
      "Recall@10 0.0465\n",
      "MAP@10 0.1378\n",
      "Recall@5 0.0265\n",
      "MAP@5 0.1756\n",
      "Recall@10 0.0479\n",
      "MAP@10 0.1395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 150/150 [01:39<00:00,  1.51it/s, train_auc=89.62%, skipped=19.50%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 0.0268\n",
      "MAP@5 0.1729\n",
      "Recall@10 0.0456\n",
      "MAP@10 0.1382\n",
      "Recall@5 0.027\n",
      "MAP@5 0.1751\n",
      "Recall@10 0.0472\n",
      "MAP@10 0.1393\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "results_swa = dict()\n",
    "\n",
    "for i in range(10): \n",
    "    results[i] = dict()\n",
    "    results_swa[i] = dict()\n",
    "\n",
    "    train_df, val_df, test_df = splitter.transform(data)\n",
    "\n",
    "    train_dataset = create_sparse_dataset(train_df)\n",
    "    train_positives = create_positives_dataset(train_df)\n",
    "    val_positives = create_positives_dataset(val_df)\n",
    "    test_positives = create_positives_dataset(test_df)\n",
    "\n",
    "    SEED = i\n",
    "    DIM = 128\n",
    "    LR_START = 1e-2\n",
    "    LR_END = 1e-3\n",
    "    EPOCHS = 150\n",
    "    RESTART_EPOCHS = 50\n",
    "    REG_FACTOR = 1e-2\n",
    "\n",
    "    set_seed(SEED)\n",
    "    model = get_model(SEED)\n",
    "    fit_callback = SWACallback()\n",
    "    model.fit(train_dataset, callback=fit_callback.callback_fn)\n",
    "\n",
    "    # usual\n",
    "    index = faiss.IndexFlatIP(DIM)\n",
    "    index.add(model.item_factors)\n",
    "    recs = index.search(model.user_factors, 50)[1]\n",
    "\n",
    "    for k in [5, 10]:\n",
    "        map_list = []\n",
    "        recall_list = []\n",
    "        for user_id, y_true in test_positives.items():\n",
    "            y_pred = [\n",
    "                item_id for item_id in recs[user_id]\n",
    "                if item_id not in train_positives.get(user_id, set())\n",
    "            ]\n",
    "\n",
    "            map_list.append(user_ap(y_pred, y_true, k))\n",
    "            recall_list.append(user_recall(y_pred, y_true, k))\n",
    "        print(f'Recall@{k}', round(np.mean(recall_list), 4))\n",
    "        print(f'MAP@{k}', round(np.nanmean(map_list), 4))\n",
    "        results[i][f'Recall@{k}'] = round(np.mean(recall_list), 4)\n",
    "        results[i][f'MAP@{k}'] = round(np.nanmean(map_list), 4)\n",
    "        \n",
    "    # swa\n",
    "    index = faiss.IndexFlatIP(DIM)\n",
    "    index.add(fit_callback.item_factors_swa)\n",
    "    recs = index.search(fit_callback.user_factors_swa, 200)[1]\n",
    "\n",
    "    for k in [5, 10]:\n",
    "        recall_list = []\n",
    "        map_list = []\n",
    "        for user_id, y_true in test_positives.items():\n",
    "            y_pred = [\n",
    "                item_id for item_id in recs[user_id]\n",
    "                if item_id not in train_positives.get(user_id, set())\n",
    "            ]\n",
    "            map_list.append(user_ap(y_pred, y_true, k))\n",
    "            recall_list.append(user_recall(y_pred, y_true, k))\n",
    "        print(f'Recall@{k}', round(np.mean(recall_list), 4))\n",
    "        print(f'MAP@{k}', round(np.nanmean(map_list), 4))\n",
    "        results_swa[i][f'Recall@{k}'] = round(np.mean(recall_list), 4)\n",
    "        results_swa[i][f'MAP@{k}'] = round(np.nanmean(map_list), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dataset = \"m1-1m\"\n",
    "\n",
    "json_dict = json.dumps(results_swa)\n",
    "with open(f\"result_swa_{dataset}.json\", \"w\") as outfile:\n",
    "    json.dump(json_dict, outfile)\n",
    "    \n",
    "json_dict = json.dumps(results)\n",
    "with open(f\"result_usual_{dataset}.json\", \"w\") as outfile:\n",
    "    json.dump(json_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Цикличный learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 150/150 [01:37<00:00,  1.55it/s, train_auc=89.41%, skipped=19.61%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 0.026\n",
      "MAP@5 0.1745\n",
      "Recall@10 0.0465\n",
      "MAP@10 0.1391\n",
      "Recall@5 0.0281\n",
      "MAP@5 0.1808\n",
      "Recall@10 0.0492\n",
      "MAP@10 0.1432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 150/150 [01:37<00:00,  1.53it/s, train_auc=89.38%, skipped=19.57%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 0.0264\n",
      "MAP@5 0.1762\n",
      "Recall@10 0.0475\n",
      "MAP@10 0.1405\n",
      "Recall@5 0.029\n",
      "MAP@5 0.1777\n",
      "Recall@10 0.0492\n",
      "MAP@10 0.1428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 150/150 [01:37<00:00,  1.54it/s, train_auc=89.39%, skipped=19.57%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 0.026\n",
      "MAP@5 0.1764\n",
      "Recall@10 0.047\n",
      "MAP@10 0.1406\n",
      "Recall@5 0.0275\n",
      "MAP@5 0.1782\n",
      "Recall@10 0.048\n",
      "MAP@10 0.1414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 150/150 [01:36<00:00,  1.55it/s, train_auc=89.40%, skipped=19.61%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 0.0263\n",
      "MAP@5 0.1747\n",
      "Recall@10 0.047\n",
      "MAP@10 0.1394\n",
      "Recall@5 0.0283\n",
      "MAP@5 0.1792\n",
      "Recall@10 0.0491\n",
      "MAP@10 0.142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 150/150 [01:38<00:00,  1.52it/s, train_auc=89.35%, skipped=19.56%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 0.0261\n",
      "MAP@5 0.1736\n",
      "Recall@10 0.046\n",
      "MAP@10 0.1383\n",
      "Recall@5 0.0287\n",
      "MAP@5 0.1763\n",
      "Recall@10 0.0488\n",
      "MAP@10 0.1412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 150/150 [01:37<00:00,  1.54it/s, train_auc=89.32%, skipped=19.46%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 0.0262\n",
      "MAP@5 0.1741\n",
      "Recall@10 0.0456\n",
      "MAP@10 0.1384\n",
      "Recall@5 0.028\n",
      "MAP@5 0.1781\n",
      "Recall@10 0.049\n",
      "MAP@10 0.1425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 150/150 [01:36<00:00,  1.56it/s, train_auc=89.38%, skipped=19.47%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 0.0271\n",
      "MAP@5 0.1732\n",
      "Recall@10 0.0463\n",
      "MAP@10 0.14\n",
      "Recall@5 0.0286\n",
      "MAP@5 0.177\n",
      "Recall@10 0.0484\n",
      "MAP@10 0.1413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 150/150 [01:38<00:00,  1.53it/s, train_auc=89.35%, skipped=19.59%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 0.0263\n",
      "MAP@5 0.1749\n",
      "Recall@10 0.0461\n",
      "MAP@10 0.1395\n",
      "Recall@5 0.0275\n",
      "MAP@5 0.1792\n",
      "Recall@10 0.0483\n",
      "MAP@10 0.1419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 150/150 [01:39<00:00,  1.51it/s, train_auc=89.47%, skipped=19.58%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 0.0256\n",
      "MAP@5 0.1697\n",
      "Recall@10 0.0461\n",
      "MAP@10 0.1378\n",
      "Recall@5 0.0281\n",
      "MAP@5 0.1776\n",
      "Recall@10 0.048\n",
      "MAP@10 0.1409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████| 150/150 [01:36<00:00,  1.55it/s, train_auc=89.38%, skipped=19.50%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 0.0268\n",
      "MAP@5 0.1733\n",
      "Recall@10 0.0457\n",
      "MAP@10 0.1388\n",
      "Recall@5 0.0276\n",
      "MAP@5 0.1787\n",
      "Recall@10 0.0491\n",
      "MAP@10 0.1423\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "results_swa = dict()\n",
    "\n",
    "for i in range(10): \n",
    "    results[i] = dict()\n",
    "    results_swa[i] = dict()\n",
    "\n",
    "    train_df, val_df, test_df = splitter.transform(data)\n",
    "\n",
    "    train_dataset = create_sparse_dataset(train_df)\n",
    "    train_positives = create_positives_dataset(train_df)\n",
    "    val_positives = create_positives_dataset(val_df)\n",
    "    test_positives = create_positives_dataset(test_df)\n",
    "\n",
    "    SEED = i\n",
    "    DIM = 128\n",
    "    LR_START = 1e-2\n",
    "    LR_END = 1e-3\n",
    "    EPOCHS = 150\n",
    "    RESTART_EPOCHS = 50\n",
    "    REG_FACTOR = 1e-2\n",
    "\n",
    "    set_seed(SEED)\n",
    "    model = get_model(SEED)\n",
    "    fit_callback = SWACallback(\"cyclic_lr\")\n",
    "    model.fit(train_dataset, callback=fit_callback.callback_fn)\n",
    "\n",
    "    # usual\n",
    "    index = faiss.IndexFlatIP(DIM)\n",
    "    index.add(model.item_factors)\n",
    "    recs = index.search(model.user_factors, 50)[1]\n",
    "\n",
    "    for k in [5, 10]:\n",
    "        map_list = []\n",
    "        recall_list = []\n",
    "        for user_id, y_true in test_positives.items():\n",
    "            y_pred = [\n",
    "                item_id for item_id in recs[user_id]\n",
    "                if item_id not in train_positives.get(user_id, set())\n",
    "            ]\n",
    "\n",
    "            map_list.append(user_ap(y_pred, y_true, k))\n",
    "            recall_list.append(user_recall(y_pred, y_true, k))\n",
    "        print(f'Recall@{k}', round(np.mean(recall_list), 4))\n",
    "        print(f'MAP@{k}', round(np.nanmean(map_list), 4))\n",
    "        results[i][f'Recall@{k}'] = round(np.mean(recall_list), 4)\n",
    "        results[i][f'MAP@{k}'] = round(np.nanmean(map_list), 4)\n",
    "        \n",
    "    # swa\n",
    "    index = faiss.IndexFlatIP(DIM)\n",
    "    index.add(fit_callback.item_factors_swa)\n",
    "    recs = index.search(fit_callback.user_factors_swa, 200)[1]\n",
    "\n",
    "    for k in [5, 10]:\n",
    "        recall_list = []\n",
    "        map_list = []\n",
    "        for user_id, y_true in test_positives.items():\n",
    "            y_pred = [\n",
    "                item_id for item_id in recs[user_id]\n",
    "                if item_id not in train_positives.get(user_id, set())\n",
    "            ]\n",
    "            map_list.append(user_ap(y_pred, y_true, k))\n",
    "            recall_list.append(user_recall(y_pred, y_true, k))\n",
    "        print(f'Recall@{k}', round(np.mean(recall_list), 4))\n",
    "        print(f'MAP@{k}', round(np.nanmean(map_list), 4))\n",
    "        results_swa[i][f'Recall@{k}'] = round(np.mean(recall_list), 4)\n",
    "        results_swa[i][f'MAP@{k}'] = round(np.nanmean(map_list), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dataset = \"m1-1m\"\n",
    "\n",
    "json_dict = json.dumps(results_swa)\n",
    "with open(f\"result_swa_cycle_lr_{dataset}.json\", \"w\") as outfile:\n",
    "    json.dump(json_dict, outfile)\n",
    "    \n",
    "json_dict = json.dumps(results)\n",
    "with open(f\"result_usual_cycle_lr_{dataset}.json\", \"w\") as outfile:\n",
    "    json.dump(json_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (121_505, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>item_id</th></tr><tr><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>11072</td><td>3741</td></tr><tr><td>11072</td><td>4804</td></tr><tr><td>11072</td><td>5032</td></tr><tr><td>11073</td><td>7622</td></tr><tr><td>11073</td><td>1295</td></tr><tr><td>11073</td><td>5406</td></tr><tr><td>11073</td><td>6400</td></tr><tr><td>11074</td><td>6000</td></tr><tr><td>11074</td><td>4381</td></tr><tr><td>11074</td><td>11937</td></tr><tr><td>11074</td><td>4685</td></tr><tr><td>11074</td><td>8965</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>11071</td><td>9720</td></tr><tr><td>11071</td><td>6761</td></tr><tr><td>11071</td><td>5713</td></tr><tr><td>11071</td><td>998</td></tr><tr><td>11071</td><td>6283</td></tr><tr><td>11071</td><td>8650</td></tr><tr><td>11071</td><td>11432</td></tr><tr><td>11071</td><td>6598</td></tr><tr><td>11071</td><td>10648</td></tr><tr><td>11071</td><td>3461</td></tr><tr><td>11071</td><td>11471</td></tr><tr><td>11071</td><td>10788</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (121_505, 2)\n",
       "┌─────────┬─────────┐\n",
       "│ user_id ┆ item_id │\n",
       "│ ---     ┆ ---     │\n",
       "│ i64     ┆ i64     │\n",
       "╞═════════╪═════════╡\n",
       "│ 11072   ┆ 3741    │\n",
       "│ 11072   ┆ 4804    │\n",
       "│ 11072   ┆ 5032    │\n",
       "│ 11073   ┆ 7622    │\n",
       "│ …       ┆ …       │\n",
       "│ 11071   ┆ 10648   │\n",
       "│ 11071   ┆ 3461    │\n",
       "│ 11071   ┆ 11471   │\n",
       "│ 11071   ┆ 10788   │\n",
       "└─────────┴─────────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = \"bcr\"\n",
    "\n",
    "from src.dataset import Dataset\n",
    "from src.postprocess_data import MinInteractionsFilter, IdsEncoder, SplitTrainValTest\n",
    "\n",
    "data = Dataset(dataset).get_data()\n",
    "\n",
    "min_iterations_filter = MinInteractionsFilter()\n",
    "ids_encoder = IdsEncoder()\n",
    "\n",
    "data = min_iterations_filter.transform(data)\n",
    "data = ids_encoder.fit_transform(data)\n",
    "\n",
    "n_users = len(data['user_id'].unique())\n",
    "n_items = len(data['item_id'].unique())\n",
    "n_interactions = len(data)\n",
    "\n",
    "splitter = SplitTrainValTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from implicit.bpr import BayesianPersonalizedRanking\n",
    "\n",
    "from src.metrics import user_recall, user_ap\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def get_model(seed):\n",
    "    return BayesianPersonalizedRanking(\n",
    "        iterations=EPOCHS, factors=(DIM - 1), random_state=seed,\n",
    "        learning_rate=LR_START, regularization=REG_FACTOR\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "negative column index found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m results_swa[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m      8\u001b[0m train_df, val_df, test_df \u001b[38;5;241m=\u001b[39m splitter\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[0;32m---> 10\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_sparse_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m train_positives \u001b[38;5;241m=\u001b[39m create_positives_dataset(train_df)\n\u001b[1;32m     12\u001b[0m val_positives \u001b[38;5;241m=\u001b[39m create_positives_dataset(val_df)\n",
      "File \u001b[0;32m~/stable-embeddings/src/postprocess_data.py:132\u001b[0m, in \u001b[0;36mcreate_sparse_dataset\u001b[0;34m(data, shape)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     shape \u001b[38;5;241m=\u001b[39m (data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcsr_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mitem_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38item/lib/python3.8/site-packages/scipy/sparse/_compressed.py:53\u001b[0m, in \u001b[0;36m_cs_matrix.__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arg1) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;66;03m# (data, ij) format\u001b[39;00m\n\u001b[1;32m     52\u001b[0m         other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\n\u001b[0;32m---> 53\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coo_container\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m         )\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_self(other)\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arg1) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;66;03m# (data, indices, indptr) format\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38item/lib/python3.8/site-packages/scipy/sparse/_coo.py:196\u001b[0m, in \u001b[0;36mcoo_matrix.__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38item/lib/python3.8/site-packages/scipy/sparse/_coo.py:289\u001b[0m, in \u001b[0;36mcoo_matrix._check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative row index found\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative column index found\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: negative column index found"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "results_swa = dict()\n",
    "\n",
    "for i in range(10): \n",
    "    results[i] = dict()\n",
    "    results_swa[i] = dict()\n",
    "\n",
    "    train_df, val_df, test_df = splitter.transform(data)\n",
    "\n",
    "    train_dataset = create_sparse_dataset(train_df)\n",
    "    train_positives = create_positives_dataset(train_df)\n",
    "    val_positives = create_positives_dataset(val_df)\n",
    "    test_positives = create_positives_dataset(test_df)\n",
    "\n",
    "    SEED = i\n",
    "    DIM = 128\n",
    "    LR_START = 1e-2\n",
    "    LR_END = 1e-3\n",
    "    EPOCHS = 150\n",
    "    RESTART_EPOCHS = 50\n",
    "    REG_FACTOR = 1e-2\n",
    "\n",
    "    set_seed(SEED)\n",
    "    model = get_model(SEED)\n",
    "    fit_callback = SWACallback()\n",
    "    model.fit(train_dataset, callback=fit_callback.callback_fn)\n",
    "\n",
    "    # usual\n",
    "    index = faiss.IndexFlatIP(DIM)\n",
    "    index.add(model.item_factors)\n",
    "    recs = index.search(model.user_factors, 50)[1]\n",
    "\n",
    "    for k in [5, 10]:\n",
    "        map_list = []\n",
    "        recall_list = []\n",
    "        for user_id, y_true in test_positives.items():\n",
    "            y_pred = [\n",
    "                item_id for item_id in recs[user_id]\n",
    "                if item_id not in train_positives.get(user_id, set())\n",
    "            ]\n",
    "\n",
    "            map_list.append(user_ap(y_pred, y_true, k))\n",
    "            recall_list.append(user_recall(y_pred, y_true, k))\n",
    "        print(f'Recall@{k}', round(np.mean(recall_list), 4))\n",
    "        print(f'MAP@{k}', round(np.nanmean(map_list), 4))\n",
    "        results[i][f'Recall@{k}'] = round(np.mean(recall_list), 4)\n",
    "        results[i][f'MAP@{k}'] = round(np.nanmean(map_list), 4)\n",
    "        \n",
    "    # swa\n",
    "    index = faiss.IndexFlatIP(DIM)\n",
    "    index.add(fit_callback.item_factors_swa)\n",
    "    recs = index.search(fit_callback.user_factors_swa, 200)[1]\n",
    "\n",
    "    for k in [5, 10]:\n",
    "        recall_list = []\n",
    "        map_list = []\n",
    "        for user_id, y_true in test_positives.items():\n",
    "            y_pred = [\n",
    "                item_id for item_id in recs[user_id]\n",
    "                if item_id not in train_positives.get(user_id, set())\n",
    "            ]\n",
    "            map_list.append(user_ap(y_pred, y_true, k))\n",
    "            recall_list.append(user_recall(y_pred, y_true, k))\n",
    "        print(f'Recall@{k}', round(np.mean(recall_list), 4))\n",
    "        print(f'MAP@{k}', round(np.nanmean(map_list), 4))\n",
    "        results_swa[i][f'Recall@{k}'] = round(np.mean(recall_list), 4)\n",
    "        results_swa[i][f'MAP@{k}'] = round(np.nanmean(map_list), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38item",
   "language": "python",
   "name": "py38item"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
